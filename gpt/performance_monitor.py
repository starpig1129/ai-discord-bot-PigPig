"""
æ€§èƒ½ç›£æ§æ¨¡çµ„

æ•´åˆæ‰€æœ‰å„ªåŒ–åŠŸèƒ½çš„ç›£æ§ã€çµ±è¨ˆå’Œç®¡ç†ï¼Œæä¾›çµ±ä¸€çš„æ€§èƒ½åˆ†æä»‹é¢ã€‚
"""

import time
import asyncio
import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import json

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™è³‡æ–™çµæ§‹"""
    timestamp: float = field(default_factory=time.time)
    response_time: float = 0.0
    gemini_cache_hits: int = 0
    gemini_cache_misses: int = 0
    processing_cache_hits: int = 0
    processing_cache_misses: int = 0
    memory_cache_hits: int = 0
    memory_cache_misses: int = 0
    parallel_tools_executed: int = 0
    parallel_execution_time: float = 0.0
    memory_search_time: float = 0.0
    api_call_count: int = 0
    api_call_time: float = 0.0
    error_count: int = 0

class PerformanceMonitor:
    """æ€§èƒ½ç›£æ§å™¨"""
    
    def __init__(self, max_history: int = 1000):
        """åˆå§‹åŒ–æ€§èƒ½ç›£æ§å™¨
        
        Args:
            max_history: æœ€å¤§æ­·å²è¨˜éŒ„æ•¸é‡
        """
        self.max_history = max_history
        self.logger = logging.getLogger(__name__)
        
        # æ€§èƒ½æ­·å²è¨˜éŒ„
        self.metrics_history: List[PerformanceMetrics] = []
        
        # ç•¶å‰æœƒè©±çµ±è¨ˆ
        self.session_start_time = time.time()
        self.total_requests = 0
        self.total_response_time = 0.0
        self.total_errors = 0
        
        # è­¦å‘Šé–¾å€¼è¨­å®š
        self.warning_thresholds = {
            'response_time': 10.0,  # å›æ‡‰æ™‚é–“è¶…é10ç§’
            'error_rate': 0.1,      # éŒ¯èª¤ç‡è¶…é10%
            'cache_hit_rate': 0.5,  # å¿«å–å‘½ä¸­ç‡ä½æ–¼50%
            'memory_search_time': 5.0  # è¨˜æ†¶æœç´¢è¶…é5ç§’
        }
        
        # æ€§èƒ½å„ªåŒ–å»ºè­°
        self.optimization_suggestions: List[str] = []
    
    def start_request_tracking(self) -> str:
        """é–‹å§‹è¿½è¹¤è«‹æ±‚
        
        Returns:
            str: è¿½è¹¤ID
        """
        tracking_id = f"req_{int(time.time() * 1000)}"
        return tracking_id
    
    def record_metrics(self, metrics: PerformanceMetrics) -> None:
        """è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
        
        Args:
            metrics: æ€§èƒ½æŒ‡æ¨™
        """
        # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
        self.metrics_history.append(metrics)
        
        # é™åˆ¶æ­·å²è¨˜éŒ„å¤§å°
        if len(self.metrics_history) > self.max_history:
            self.metrics_history.pop(0)
        
        # æ›´æ–°æœƒè©±çµ±è¨ˆ
        self.total_requests += 1
        self.total_response_time += metrics.response_time
        self.total_errors += metrics.error_count
        
        # æª¢æŸ¥è­¦å‘Šæ¢ä»¶
        self._check_performance_warnings(metrics)
        
        self.logger.debug(f"è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™: å›æ‡‰æ™‚é–“ {metrics.response_time:.2f}s")
    
    def get_current_stats(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰çµ±è¨ˆè³‡è¨Š
        
        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        if not self.metrics_history:
            return {"status": "no_data"}
        
        recent_metrics = self.metrics_history[-10:]  # æœ€è¿‘10æ¬¡è«‹æ±‚
        
        # è¨ˆç®—å¹³å‡å€¼
        avg_response_time = sum(m.response_time for m in recent_metrics) / len(recent_metrics)
        
        # è¨ˆç®—å¿«å–å‘½ä¸­ç‡
        total_cache_hits = sum(
            m.gemini_cache_hits + m.processing_cache_hits + m.memory_cache_hits
            for m in recent_metrics
        )
        total_cache_requests = sum(
            m.gemini_cache_hits + m.gemini_cache_misses +
            m.processing_cache_hits + m.processing_cache_misses +
            m.memory_cache_hits + m.memory_cache_misses
            for m in recent_metrics
        )
        
        cache_hit_rate = (total_cache_hits / total_cache_requests) if total_cache_requests > 0 else 0
        
        # è¨ˆç®—éŒ¯èª¤ç‡
        total_errors = sum(m.error_count for m in recent_metrics)
        error_rate = total_errors / len(recent_metrics)
        
        # æœƒè©±çµ±è¨ˆ
        session_duration = time.time() - self.session_start_time
        session_avg_response_time = (
            self.total_response_time / self.total_requests
            if self.total_requests > 0 else 0
        )
        session_error_rate = self.total_errors / self.total_requests if self.total_requests > 0 else 0
        
        return {
            "current_performance": {
                "average_response_time": f"{avg_response_time:.2f}s",
                "cache_hit_rate": f"{cache_hit_rate * 100:.1f}%",
                "error_rate": f"{error_rate * 100:.1f}%",
                "recent_requests": len(recent_metrics)
            },
            "session_statistics": {
                "duration": f"{session_duration / 3600:.1f}h",
                "total_requests": self.total_requests,
                "average_response_time": f"{session_avg_response_time:.2f}s",
                "error_rate": f"{session_error_rate * 100:.1f}%"
            },
            "optimization_suggestions": self.optimization_suggestions[-5:],  # æœ€è¿‘5å€‹å»ºè­°
            "last_updated": datetime.now().isoformat()
        }
    
    def get_detailed_analytics(self) -> Dict[str, Any]:
        """ç²å–è©³ç´°åˆ†æå ±å‘Š
        
        Returns:
            è©³ç´°åˆ†æå ±å‘Š
        """
        if not self.metrics_history:
            return {"status": "no_data"}
        
        # æ™‚é–“ç¯„åœåˆ†æ
        recent_1h = [m for m in self.metrics_history if time.time() - m.timestamp < 3600]
        recent_24h = [m for m in self.metrics_history if time.time() - m.timestamp < 86400]
        
        def analyze_metrics(metrics_list: List[PerformanceMetrics], period_name: str) -> Dict[str, Any]:
            if not metrics_list:
                return {}
            
            response_times = [m.response_time for m in metrics_list]
            api_times = [m.api_call_time for m in metrics_list]
            memory_times = [m.memory_search_time for m in metrics_list]
            
            return {
                f"{period_name}_requests": len(metrics_list),
                f"{period_name}_avg_response_time": f"{sum(response_times) / len(response_times):.2f}s",
                f"{period_name}_max_response_time": f"{max(response_times):.2f}s",
                f"{period_name}_min_response_time": f"{min(response_times):.2f}s",
                f"{period_name}_avg_api_time": f"{sum(api_times) / len(api_times):.2f}s",
                f"{period_name}_avg_memory_time": f"{sum(memory_times) / len(memory_times):.2f}s",
            }
        
        analytics = {
            "overview": {
                "total_recorded_requests": len(self.metrics_history),
                "monitoring_duration": f"{(time.time() - self.session_start_time) / 3600:.1f}h"
            }
        }
        
        # æ·»åŠ ä¸åŒæ™‚é–“ç¯„åœçš„åˆ†æ
        analytics.update(analyze_metrics(recent_1h, "1h"))
        analytics.update(analyze_metrics(recent_24h, "24h"))
        analytics.update(analyze_metrics(self.metrics_history, "all_time"))
        
        # å¿«å–æ•ˆèƒ½åˆ†æ
        if recent_1h:
            gemini_hits = sum(m.gemini_cache_hits for m in recent_1h)
            gemini_misses = sum(m.gemini_cache_misses for m in recent_1h)
            gemini_total = gemini_hits + gemini_misses
            
            processing_hits = sum(m.processing_cache_hits for m in recent_1h)
            processing_misses = sum(m.processing_cache_misses for m in recent_1h)
            processing_total = processing_hits + processing_misses
            
            memory_hits = sum(m.memory_cache_hits for m in recent_1h)
            memory_misses = sum(m.memory_cache_misses for m in recent_1h)
            memory_total = memory_hits + memory_misses
            
            analytics["cache_performance"] = {
                "gemini_cache_hit_rate": f"{(gemini_hits / gemini_total * 100):.1f}%" if gemini_total > 0 else "N/A",
                "processing_cache_hit_rate": f"{(processing_hits / processing_total * 100):.1f}%" if processing_total > 0 else "N/A",
                "memory_cache_hit_rate": f"{(memory_hits / memory_total * 100):.1f}%" if memory_total > 0 else "N/A"
            }
        
        return analytics
    
    def _check_performance_warnings(self, metrics: PerformanceMetrics) -> None:
        """æª¢æŸ¥æ€§èƒ½è­¦å‘Šæ¢ä»¶
        
        Args:
            metrics: ç•¶å‰æ€§èƒ½æŒ‡æ¨™
        """
        suggestions = []
        
        # æª¢æŸ¥å›æ‡‰æ™‚é–“
        if metrics.response_time > self.warning_thresholds['response_time']:
            suggestions.append(f"å›æ‡‰æ™‚é–“éé•· ({metrics.response_time:.1f}s)ï¼Œå»ºè­°æª¢æŸ¥APIèª¿ç”¨å’Œè¨˜æ†¶æœç´¢æ•ˆç‡")
        
        # æª¢æŸ¥è¨˜æ†¶æœç´¢æ™‚é–“
        if metrics.memory_search_time > self.warning_thresholds['memory_search_time']:
            suggestions.append(f"è¨˜æ†¶æœç´¢è€—æ™‚éé•· ({metrics.memory_search_time:.1f}s)ï¼Œå»ºè­°å•Ÿç”¨é è¼‰å…¥æˆ–å„ªåŒ–ç´¢å¼•")
        
        # æª¢æŸ¥å¿«å–å‘½ä¸­ç‡
        total_cache_requests = (
            metrics.gemini_cache_hits + metrics.gemini_cache_misses +
            metrics.processing_cache_hits + metrics.processing_cache_misses +
            metrics.memory_cache_hits + metrics.memory_cache_misses
        )
        
        if total_cache_requests > 0:
            total_cache_hits = (
                metrics.gemini_cache_hits + 
                metrics.processing_cache_hits + 
                metrics.memory_cache_hits
            )
            cache_hit_rate = total_cache_hits / total_cache_requests
            
            if cache_hit_rate < self.warning_thresholds['cache_hit_rate']:
                suggestions.append(f"å¿«å–å‘½ä¸­ç‡åä½ ({cache_hit_rate * 100:.1f}%)ï¼Œå»ºè­°èª¿æ•´å¿«å–ç­–ç•¥æˆ–TTLè¨­å®š")
        
        # æª¢æŸ¥éŒ¯èª¤ç‡
        if len(self.metrics_history) >= 10:
            recent_errors = sum(m.error_count for m in self.metrics_history[-10:])
            error_rate = recent_errors / 10
            
            if error_rate > self.warning_thresholds['error_rate']:
                suggestions.append(f"éŒ¯èª¤ç‡åé«˜ ({error_rate * 100:.1f}%)ï¼Œè«‹æª¢æŸ¥ç³»çµ±ç©©å®šæ€§")
        
        # æ·»åŠ å»ºè­°åˆ°åˆ—è¡¨
        for suggestion in suggestions:
            if suggestion not in self.optimization_suggestions:
                self.optimization_suggestions.append(suggestion)
                self.logger.warning(f"æ€§èƒ½è­¦å‘Š: {suggestion}")
        
        # é™åˆ¶å»ºè­°åˆ—è¡¨å¤§å°
        if len(self.optimization_suggestions) > 20:
            self.optimization_suggestions = self.optimization_suggestions[-20:]
    
    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½å ±å‘Š
        
        Returns:
            æ ¼å¼åŒ–çš„æ€§èƒ½å ±å‘Šå­—ä¸²
        """
        stats = self.get_current_stats()
        analytics = self.get_detailed_analytics()
        
        report_lines = [
            "=== Discord æ©Ÿå™¨äººæ€§èƒ½å ±å‘Š ===",
            f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "ğŸ“Š ç•¶å‰æ€§èƒ½æŒ‡æ¨™:",
            f"  â€¢ å¹³å‡å›æ‡‰æ™‚é–“: {stats['current_performance']['average_response_time']}",
            f"  â€¢ å¿«å–å‘½ä¸­ç‡: {stats['current_performance']['cache_hit_rate']}",
            f"  â€¢ éŒ¯èª¤ç‡: {stats['current_performance']['error_rate']}",
            "",
            "ğŸ“ˆ æœƒè©±çµ±è¨ˆ:",
            f"  â€¢ é‹è¡Œæ™‚é–“: {stats['session_statistics']['duration']}",
            f"  â€¢ ç¸½è«‹æ±‚æ•¸: {stats['session_statistics']['total_requests']}",
            f"  â€¢ å¹³å‡å›æ‡‰æ™‚é–“: {stats['session_statistics']['average_response_time']}",
            "",
        ]
        
        if analytics.get("cache_performance"):
            report_lines.extend([
                "ğŸš€ å¿«å–æ€§èƒ½ (æœ€è¿‘1å°æ™‚):",
                f"  â€¢ Gemini API å¿«å–: {analytics['cache_performance']['gemini_cache_hit_rate']}",
                f"  â€¢ è™•ç†çµæœå¿«å–: {analytics['cache_performance']['processing_cache_hit_rate']}",
                f"  â€¢ è¨˜æ†¶æœç´¢å¿«å–: {analytics['cache_performance']['memory_cache_hit_rate']}",
                ""
            ])
        
        if stats.get("optimization_suggestions"):
            report_lines.extend([
                "âš ï¸  å„ªåŒ–å»ºè­°:",
                *[f"  â€¢ {suggestion}" for suggestion in stats["optimization_suggestions"]],
                ""
            ])
        
        report_lines.append("=== å ±å‘ŠçµæŸ ===")
        
        return "\n".join(report_lines)
    
    def export_metrics(self, filepath: str = None) -> str:
        """åŒ¯å‡ºæ€§èƒ½æŒ‡æ¨™åˆ°JSONæª”æ¡ˆ
        
        Args:
            filepath: åŒ¯å‡ºæª”æ¡ˆè·¯å¾‘
            
        Returns:
            åŒ¯å‡ºæª”æ¡ˆè·¯å¾‘
        """
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"performance_metrics_{timestamp}.json"
        
        export_data = {
            "export_time": datetime.now().isoformat(),
            "session_info": {
                "start_time": self.session_start_time,
                "total_requests": self.total_requests,
                "total_response_time": self.total_response_time,
                "total_errors": self.total_errors
            },
            "metrics_history": [
                {
                    "timestamp": m.timestamp,
                    "response_time": m.response_time,
                    "gemini_cache_hits": m.gemini_cache_hits,
                    "gemini_cache_misses": m.gemini_cache_misses,
                    "processing_cache_hits": m.processing_cache_hits,
                    "processing_cache_misses": m.processing_cache_misses,
                    "memory_cache_hits": m.memory_cache_hits,
                    "memory_cache_misses": m.memory_cache_misses,
                    "parallel_tools_executed": m.parallel_tools_executed,
                    "parallel_execution_time": m.parallel_execution_time,
                    "memory_search_time": m.memory_search_time,
                    "api_call_count": m.api_call_count,
                    "api_call_time": m.api_call_time,
                    "error_count": m.error_count
                }
                for m in self.metrics_history
            ],
            "optimization_suggestions": self.optimization_suggestions
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"æ€§èƒ½æŒ‡æ¨™å·²åŒ¯å‡ºè‡³: {filepath}")
            return filepath
            
        except Exception as e:
            self.logger.error(f"åŒ¯å‡ºæ€§èƒ½æŒ‡æ¨™å¤±æ•—: {str(e)}")
            raise
    
    def reset_metrics(self) -> None:
        """é‡ç½®æ‰€æœ‰æ€§èƒ½æŒ‡æ¨™"""
        self.metrics_history.clear()
        self.session_start_time = time.time()
        self.total_requests = 0
        self.total_response_time = 0.0
        self.total_errors = 0
        self.optimization_suggestions.clear()
        
        self.logger.info("æ€§èƒ½æŒ‡æ¨™å·²é‡ç½®")

# å…¨åŸŸæ€§èƒ½ç›£æ§å™¨å¯¦ä¾‹
performance_monitor = PerformanceMonitor()

# ä¾¿æ·å‡½æ•¸
def record_performance_metrics(metrics: PerformanceMetrics) -> None:
    """ä¾¿æ·å‡½æ•¸ï¼šè¨˜éŒ„æ€§èƒ½æŒ‡æ¨™"""
    performance_monitor.record_metrics(metrics)

def get_performance_stats() -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•¸ï¼šç²å–æ€§èƒ½çµ±è¨ˆ"""
    return performance_monitor.get_current_stats()

def get_performance_report() -> str:
    """ä¾¿æ·å‡½æ•¸ï¼šç²å–æ€§èƒ½å ±å‘Š"""
    return performance_monitor.generate_performance_report()

def create_performance_metrics(**kwargs) -> PerformanceMetrics:
    """ä¾¿æ·å‡½æ•¸ï¼šå‰µå»ºæ€§èƒ½æŒ‡æ¨™"""
    return PerformanceMetrics(**kwargs)