# Qwen3 æ¨¡å‹é·ç§»æ•…éšœæ’é™¤æŒ‡å—

æœ¬æŒ‡å—æä¾› Qwen3 æ¨¡å‹é·ç§»éç¨‹ä¸­å¸¸è¦‹å•é¡Œçš„è¨ºæ–·å’Œè§£æ±ºæ–¹æ¡ˆã€‚

## ç›®éŒ„

1. [å¸¸è¦‹éŒ¯èª¤èˆ‡è§£æ±ºæ–¹æ¡ˆ](#å¸¸è¦‹éŒ¯èª¤èˆ‡è§£æ±ºæ–¹æ¡ˆ)
2. [ç³»çµ±è¨ºæ–·å·¥å…·](#ç³»çµ±è¨ºæ–·å·¥å…·)
3. [æ—¥èªŒåˆ†æ](#æ—¥èªŒåˆ†æ)
4. [æ•ˆèƒ½å•é¡Œ](#æ•ˆèƒ½å•é¡Œ)
5. [è³‡æ–™å®Œæ•´æ€§å•é¡Œ](#è³‡æ–™å®Œæ•´æ€§å•é¡Œ)
6. [ç·Šæ€¥æ¢å¾©ç¨‹åº](#ç·Šæ€¥æ¢å¾©ç¨‹åº)

## å¸¸è¦‹éŒ¯èª¤èˆ‡è§£æ±ºæ–¹æ¡ˆ

### 1. è¨˜æ†¶é«”ç›¸é—œéŒ¯èª¤

#### OutOfMemoryError
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**åŸå› **: GPU è¨˜æ†¶é«”ä¸è¶³

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ 1: æ¸›å°‘æ‰¹æ¬¡å¤§å°
python migrate_to_qwen3.py --migrate --batch-size=10

# æ–¹æ¡ˆ 2: å•Ÿç”¨ CPU æ¨¡å¼
# ç·¨è¼¯ settings.json
{
  "memory_system": {
    "cpu_only_mode": true
  }
}

# æ–¹æ¡ˆ 3: å¢åŠ  GPU è¨˜æ†¶é«”é™åˆ¶
{
  "memory_system": {
    "hardware_detection": {
      "gpu_memory_limit_mb": 1024,
      "fallback_to_cpu": true
    }
  }
}
```

#### MemoryError: Unable to allocate array
```
MemoryError: Unable to allocate 4.00 GiB for an array
```

**åŸå› **: ç³»çµ± RAM ä¸è¶³

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# é—œé–‰å…¶ä»–ç¨‹åºé‡‹æ”¾è¨˜æ†¶é«”
sudo systemctl stop unnecessary-services

# ä½¿ç”¨è¼ƒå°æ‰¹æ¬¡å¤§å°
python migrate_to_qwen3.py --migrate --batch-size=5

# å¢åŠ è™›æ“¬è¨˜æ†¶é«” (Linux)
sudo swapon --show
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### 2. æ¨¡å‹è¼‰å…¥éŒ¯èª¤

#### æ¨¡å‹ä¸‹è¼‰å¤±æ•—
```
OSError: Can't load tokenizer for 'Qwen/Qwen3-Embedding-0.6B'
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ 1: æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹
python -c "
from transformers import AutoTokenizer, AutoModel
import os
os.environ['HF_HOME'] = './data/memory/models'
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B')
model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')
print('æ¨¡å‹ä¸‹è¼‰å®Œæˆ')
"

# æ–¹æ¡ˆ 2: ä½¿ç”¨é¡åƒç«™é»
export HF_ENDPOINT=https://hf-mirror.com
python migrate_to_qwen3.py --migrate

# æ–¹æ¡ˆ 3: é›¢ç·šæ¨¡å¼ï¼ˆéœ€è¦é å…ˆä¸‹è¼‰çš„æ¨¡å‹ï¼‰
{
  "memory_system": {
    "embedding_model": "./data/memory/models/Qwen3-Embedding-0.6B"
  }
}
```

#### æ¨¡å‹ç‰ˆæœ¬ä¸ç›¸å®¹
```
ValueError: Model version mismatch
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æ¸…ç†å¿«å–
rm -rf ~/.cache/huggingface/
rm -rf ./data/memory/models/

# é‡æ–°ä¸‹è¼‰æœ€æ–°ç‰ˆæœ¬
python migrate_to_qwen3.py --migrate
```

### 3. è³‡æ–™åº«éŒ¯èª¤

#### è³‡æ–™åº«é–å®š
```
sqlite3.OperationalError: database is locked
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æª¢æŸ¥æ˜¯å¦æœ‰ç¨‹åºåœ¨ä½¿ç”¨è³‡æ–™åº«
lsof data/memory/memory.db

# çµ‚æ­¢ç›¸é—œç¨‹åº
pkill -f "python.*memory"

# æª¢æŸ¥è³‡æ–™åº«å®Œæ•´æ€§
sqlite3 data/memory/memory.db "PRAGMA integrity_check;"

# å¦‚æœè³‡æ–™åº«æå£ï¼Œå¾å‚™ä»½æ¢å¾©
cp data/memory/memory.db.backup data/memory/memory.db
```

#### è³‡æ–™åº«æ¶æ§‹éŒ¯èª¤
```
sqlite3.OperationalError: no such table: embeddings
```

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# é‡æ–°åˆå§‹åŒ–è³‡æ–™åº«
from cogs.memory.database import DatabaseManager

db_manager = DatabaseManager("data/memory/memory.db")
# è³‡æ–™åº«å°‡è‡ªå‹•é‡å»ºè¡¨æ ¼
```

### 4. é…ç½®æª”æ¡ˆéŒ¯èª¤

#### JSON æ ¼å¼éŒ¯èª¤
```
json.JSONDecodeError: Expecting ',' delimiter
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# é©—è­‰ JSON æ ¼å¼
python -m json.tool settings.json

# ä½¿ç”¨ç¯„ä¾‹é…ç½®
cp settings_qwen3_example.json settings.json

# æˆ–é‡æ–°ç”Ÿæˆé…ç½®
python migrate_to_qwen3.py --migrate  # æœƒè‡ªå‹•å‰µå»ºæœ‰æ•ˆé…ç½®
```

#### é…ç½®åƒæ•¸éŒ¯èª¤
```
ConfigurationError: Invalid configuration parameter
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æª¢æŸ¥é…ç½®æª”æ¡ˆ
python -c "
from cogs.memory.config import MemoryConfig
try:
    config = MemoryConfig('settings.json')
    print('é…ç½®æª”æ¡ˆæœ‰æ•ˆ')
except Exception as e:
    print(f'é…ç½®éŒ¯èª¤: {e}')
"

# æ¢å¾©é è¨­é…ç½®
{
  "memory_system": {
    "enabled": true,
    "auto_detection": true,
    "profile": "qwen3_medium_performance"
  }
}
```

### 5. å‘é‡é·ç§»éŒ¯èª¤

#### ç¶­åº¦ä¸åŒ¹é…
```
VectorOperationError: Vector dimension mismatch: expected 1536, got 768
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨ regenerate ç­–ç•¥ï¼ˆæ¨è–¦ï¼‰
python migrate_to_qwen3.py --migrate --strategy=regenerate

# æˆ–ä½¿ç”¨è½‰æ›ç­–ç•¥
python migrate_to_qwen3.py --migrate --strategy=transform
```

#### é·ç§»é€²åº¦åœæ­¢
```
INFO - é·ç§»é€²åº¦: 45.2% (stuck)
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æª¢æŸ¥ç³»çµ±è³‡æº
htop
nvidia-smi  # å¦‚æœæœ‰ GPU

# é‡æ–°å•Ÿå‹•é·ç§»ï¼ˆæœƒå¾ä¸­æ–·é»ç¹¼çºŒï¼‰
python migrate_to_qwen3.py --migrate --batch-size=25

# æª¢æŸ¥æ—¥èªŒ
tail -f logs/qwen3_migration_*.log
```

### 6. ä¾è³´å¥—ä»¶å•é¡Œ

#### PyTorch ç‰ˆæœ¬ä¸ç›¸å®¹
```
ImportError: cannot import name 'xxx' from 'torch'
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æ›´æ–° PyTorch
pip install --upgrade torch torchvision torchaudio

# æˆ–å®‰è£ç‰¹å®šç‰ˆæœ¬
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118
```

#### Transformers ç‰ˆæœ¬éèˆŠ
```
AttributeError: module 'transformers' has no attribute 'AutoModel'
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æ›´æ–° transformers
pip install --upgrade transformers>=4.30.0

# æª¢æŸ¥ç‰ˆæœ¬
python -c "import transformers; print(transformers.__version__)"
```

## ç³»çµ±è¨ºæ–·å·¥å…·

### å®Œæ•´ç³»çµ±è¨ºæ–·è…³æœ¬

```python
#!/usr/bin/env python3
"""ç³»çµ±è¨ºæ–·è…³æœ¬"""

import json
import logging
import sqlite3
import sys
from pathlib import Path

def diagnose_system():
    """åŸ·è¡Œå®Œæ•´ç³»çµ±è¨ºæ–·"""
    print("ğŸ” é–‹å§‹ç³»çµ±è¨ºæ–·...")
    
    issues = []
    
    # 1. æª¢æŸ¥é…ç½®æª”æ¡ˆ
    config_file = Path("settings.json")
    if not config_file.exists():
        issues.append("âŒ settings.json ä¸å­˜åœ¨")
    else:
        try:
            with open(config_file, "r") as f:
                json.load(f)
            print("âœ… é…ç½®æª”æ¡ˆæ ¼å¼æ­£ç¢º")
        except json.JSONDecodeError as e:
            issues.append(f"âŒ é…ç½®æª”æ¡ˆ JSON æ ¼å¼éŒ¯èª¤: {e}")
    
    # 2. æª¢æŸ¥è³‡æ–™åº«
    db_file = Path("data/memory/memory.db")
    if not db_file.exists():
        print("âš ï¸ è³‡æ–™åº«ä¸å­˜åœ¨ï¼ˆé¦–æ¬¡é‹è¡Œæ™‚æ­£å¸¸ï¼‰")
    else:
        try:
            conn = sqlite3.connect(str(db_file))
            cursor = conn.execute("SELECT COUNT(*) FROM embeddings")
            count = cursor.fetchone()[0]
            print(f"âœ… è³‡æ–™åº«å¯è¨ªå•ï¼ŒåŒ…å« {count} å€‹å‘é‡")
            conn.close()
        except Exception as e:
            issues.append(f"âŒ è³‡æ–™åº«éŒ¯èª¤: {e}")
    
    # 3. æª¢æŸ¥ä¾è³´å¥—ä»¶
    dependencies = {
        "torch": "PyTorch",
        "transformers": "Transformers",
        "sentence_transformers": "SentenceTransformers",
        "faiss": "FAISS",
        "sklearn": "Scikit-learn"
    }
    
    for pkg, name in dependencies.items():
        try:
            __import__(pkg)
            print(f"âœ… {name} å·²å®‰è£")
        except ImportError:
            issues.append(f"âŒ {name} æœªå®‰è£")
    
    # 4. æª¢æŸ¥ GPU
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            print(f"âœ… GPU å¯ç”¨: {gpu_name} (å…± {gpu_count} å€‹)")
        else:
            print("âš ï¸ GPU ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼")
    except:
        issues.append("âŒ PyTorch GPU æª¢æ¸¬å¤±æ•—")
    
    # 5. æª¢æŸ¥è¨˜æ†¶é«”
    try:
        import psutil
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        total_gb = memory.total / (1024**3)
        print(f"âœ… è¨˜æ†¶é«”: {available_gb:.1f}GB å¯ç”¨ / {total_gb:.1f}GB ç¸½é‡")
        
        if available_gb < 4:
            issues.append("âš ï¸ å¯ç”¨è¨˜æ†¶é«”ä¸è¶³ 4GB")
    except:
        issues.append("âŒ è¨˜æ†¶é«”æª¢æ¸¬å¤±æ•—")
    
    # ç¸½çµ
    print("\n" + "="*50)
    if issues:
        print("ç™¼ç¾ä»¥ä¸‹å•é¡Œ:")
        for issue in issues:
            print(f"  {issue}")
    else:
        print("âœ… ç³»çµ±æª¢æŸ¥é€šéï¼Œå¯ä»¥é€²è¡Œé·ç§»")
    
    return len(issues) == 0

if __name__ == "__main__":
    success = diagnose_system()
    sys.exit(0 if success else 1)
```

### è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§

```python
#!/usr/bin/env python3
"""è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§è…³æœ¬"""

import psutil
import time
import torch

def monitor_memory(duration=60):
    """ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨"""
    print(f"ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨ {duration} ç§’...")
    
    for i in range(duration):
        # ç³»çµ±è¨˜æ†¶é«”
        memory = psutil.virtual_memory()
        used_gb = (memory.total - memory.available) / (1024**3)
        total_gb = memory.total / (1024**3)
        
        # GPU è¨˜æ†¶é«”
        gpu_info = ""
        if torch.cuda.is_available():
            gpu_used = torch.cuda.memory_allocated() / (1024**3)
            gpu_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            gpu_info = f", GPU: {gpu_used:.1f}GB/{gpu_total:.1f}GB"
        
        print(f"\rè¨˜æ†¶é«”ä½¿ç”¨: {used_gb:.1f}GB/{total_gb:.1f}GB{gpu_info}", end="")
        time.sleep(1)
    
    print("\nç›£æ§å®Œæˆ")

if __name__ == "__main__":
    monitor_memory()
```

## æ—¥èªŒåˆ†æ

### å¸¸è¦‹æ—¥èªŒæ¨¡å¼

#### æ­£å¸¸é·ç§»æ—¥èªŒ
```
2025-07-06 01:00:00 - INFO - é–‹å§‹å‘é‡é·ç§»: paraphrase-multilingual-mpnet-base-v2 -> Qwen/Qwen3-Embedding-0.6B
2025-07-06 01:00:01 - INFO - ç¶­åº¦è®ŠåŒ–: 768 -> 1536
2025-07-06 01:00:02 - INFO - ç¸½å‘é‡æ•¸: 15420
2025-07-06 01:00:03 - INFO - é·ç§»ç­–ç•¥: regenerate
2025-07-06 01:05:30 - INFO - é·ç§»é€²åº¦: 50.0%
2025-07-06 01:10:45 - INFO - å‘é‡é·ç§»å®Œæˆ: æˆåŠŸ 15420/15420, æˆåŠŸç‡ 100.00%
```

#### è¨˜æ†¶é«”ä¸è¶³æ—¥èªŒ
```
2025-07-06 01:00:00 - ERROR - æ‰¹æ¬¡é·ç§»å¤±æ•— (offset=1000): CUDA out of memory
2025-07-06 01:00:01 - WARNING - GPU è¨˜æ†¶é«”ä¸è¶³ï¼Œåˆ‡æ›åˆ° CPU æ¨¡å¼
2025-07-06 01:00:02 - INFO - ç¹¼çºŒä½¿ç”¨ CPU é€²è¡Œé·ç§»...
```

#### æ¨¡å‹è¼‰å…¥å¤±æ•—æ—¥èªŒ
```
2025-07-06 01:00:00 - ERROR - æ¨¡å‹è¼‰å…¥å¤±æ•—: Can't load tokenizer
2025-07-06 01:00:01 - INFO - å˜—è©¦å¾å¿«å–è¼‰å…¥æ¨¡å‹...
2025-07-06 01:00:02 - INFO - é–‹å§‹ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ...
```

### æ—¥èªŒåˆ†æè…³æœ¬

```bash
#!/bin/bash
# æ—¥èªŒåˆ†æè…³æœ¬

LOG_FILE="logs/qwen3_migration_$(date +%Y%m%d)*.log"

echo "=== é·ç§»é€²åº¦åˆ†æ ==="
grep "é·ç§»é€²åº¦" $LOG_FILE | tail -10

echo -e "\n=== éŒ¯èª¤è¨Šæ¯ ==="
grep -i "error\|failed\|exception" $LOG_FILE | tail -5

echo -e "\n=== è­¦å‘Šè¨Šæ¯ ==="
grep -i "warning\|warn" $LOG_FILE | tail -5

echo -e "\n=== æ•ˆèƒ½çµ±è¨ˆ ==="
grep -E "æˆåŠŸç‡|è€—æ™‚|throughput" $LOG_FILE | tail -3
```

## æ•ˆèƒ½å•é¡Œ

### é·ç§»é€Ÿåº¦éæ…¢

**ç—‡ç‹€**: é·ç§»é€Ÿåº¦ < 100 å‘é‡/åˆ†é˜

**è¨ºæ–·æ­¥é©Ÿ**:
```bash
# 1. æª¢æŸ¥ç³»çµ±è³‡æº
htop
iotop
nvidia-smi

# 2. æª¢æŸ¥æ‰¹æ¬¡å¤§å°
grep "batch_size" settings.json

# 3. æª¢æŸ¥ I/O ä½¿ç”¨ç‡
iostat -x 1
```

**è§£æ±ºæ–¹æ¡ˆ**:
```json
{
  "memory_system": {
    "performance": {
      "batch_size": 32,
      "max_concurrent_queries": 16
    },
    "migration": {
      "migration_batch_size": 50
    }
  }
}
```

### è¨˜æ†¶é«”æ´©æ¼

**ç—‡ç‹€**: è¨˜æ†¶é«”ä½¿ç”¨æŒçºŒå¢é•·

**è¨ºæ–·è…³æœ¬**:
```python
import gc
import psutil
import time

def check_memory_leak():
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024
    
    for i in range(10):
        # æ¨¡æ“¬ä¸€äº›æ“ä½œ
        time.sleep(30)
        gc.collect()
        
        current_memory = process.memory_info().rss / 1024 / 1024
        growth = current_memory - initial_memory
        
        print(f"é€±æœŸ {i+1}: {current_memory:.1f}MB (+{growth:.1f}MB)")
        
        if growth > 500:  # 500MB å¢é•·è¦–ç‚ºå¯èƒ½çš„è¨˜æ†¶é«”æ´©æ¼
            print("âš ï¸ æª¢æ¸¬åˆ°å¯èƒ½çš„è¨˜æ†¶é«”æ´©æ¼")
            break
```

## è³‡æ–™å®Œæ•´æ€§å•é¡Œ

### å‘é‡æ•¸é‡ä¸åŒ¹é…

**è¨ºæ–·æŸ¥è©¢**:
```sql
-- æª¢æŸ¥ä¸åŒæ¨¡å‹çš„å‘é‡æ•¸é‡
SELECT model_version, COUNT(*) as count, AVG(dimension) as avg_dim
FROM embeddings 
GROUP BY model_version;

-- æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡å‘é‡
SELECT message_id, COUNT(*) as count
FROM embeddings 
GROUP BY message_id 
HAVING COUNT(*) > 1;

-- æª¢æŸ¥å‘é‡ç¶­åº¦ä¸€è‡´æ€§
SELECT DISTINCT dimension, COUNT(*) 
FROM embeddings 
GROUP BY dimension;
```

### è³‡æ–™ä¸€è‡´æ€§é©—è­‰è…³æœ¬

```python
#!/usr/bin/env python3
"""è³‡æ–™ä¸€è‡´æ€§é©—è­‰è…³æœ¬"""

import sqlite3
from pathlib import Path

def verify_data_consistency():
    """é©—è­‰è³‡æ–™ä¸€è‡´æ€§"""
    db_path = Path("data/memory/memory.db")
    if not db_path.exists():
        print("âŒ è³‡æ–™åº«ä¸å­˜åœ¨")
        return False
    
    issues = []
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # 1. æª¢æŸ¥å­¤ç«‹å‘é‡
        cursor.execute("""
            SELECT COUNT(*) FROM embeddings e 
            LEFT JOIN messages m ON e.message_id = m.message_id 
            WHERE m.message_id IS NULL
        """)
        orphaned_vectors = cursor.fetchone()[0]
        if orphaned_vectors > 0:
            issues.append(f"ç™¼ç¾ {orphaned_vectors} å€‹å­¤ç«‹å‘é‡")
        
        # 2. æª¢æŸ¥ç¼ºå¤±å‘é‡
        cursor.execute("""
            SELECT COUNT(*) FROM messages m 
            LEFT JOIN embeddings e ON m.message_id = e.message_id 
            WHERE e.message_id IS NULL
        """)
        missing_vectors = cursor.fetchone()[0]
        if missing_vectors > 0:
            issues.append(f"ç™¼ç¾ {missing_vectors} å€‹ç¼ºå¤±å‘é‡")
        
        # 3. æª¢æŸ¥ç¶­åº¦ä¸€è‡´æ€§
        cursor.execute("""
            SELECT model_version, dimension, COUNT(*) 
            FROM embeddings 
            GROUP BY model_version, dimension
        """)
        dimension_info = cursor.fetchall()
        
        print("ç¶­åº¦åˆ†ä½ˆ:")
        for model, dim, count in dimension_info:
            print(f"  {model}: {dim}ç¶­ ({count} å€‹)")
        
        conn.close()
        
        if issues:
            print("ç™¼ç¾è³‡æ–™ä¸€è‡´æ€§å•é¡Œ:")
            for issue in issues:
                print(f"  âŒ {issue}")
            return False
        else:
            print("âœ… è³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥é€šé")
            return True
            
    except Exception as e:
        print(f"âŒ è³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    verify_data_consistency()
```

## ç·Šæ€¥æ¢å¾©ç¨‹åº

### å¿«é€Ÿå›æ»¾

```bash
#!/bin/bash
# ç·Šæ€¥å›æ»¾è…³æœ¬

echo "ğŸš¨ åŸ·è¡Œç·Šæ€¥å›æ»¾..."

# 1. åœæ­¢æ‰€æœ‰ç›¸é—œç¨‹åº
pkill -f "python.*memory"
pkill -f "migrate_to_qwen3"

# 2. æ¢å¾©é…ç½®æª”æ¡ˆ
if [ -f "settings.json.backup" ]; then
    cp settings.json.backup settings.json
    echo "âœ… é…ç½®æª”æ¡ˆå·²æ¢å¾©"
fi

# 3. æ¢å¾©è³‡æ–™åº«
if [ -f "data/memory/memory.db.backup" ]; then
    cp data/memory/memory.db.backup data/memory/memory.db
    echo "âœ… è³‡æ–™åº«å·²æ¢å¾©"
fi

# 4. æ¢å¾©å‘é‡ç´¢å¼•
if [ -d "data/memory/vectors.backup" ]; then
    rm -rf data/memory/vectors
    cp -r data/memory/vectors.backup data/memory/vectors
    echo "âœ… å‘é‡ç´¢å¼•å·²æ¢å¾©"
fi

echo "ğŸ‰ ç·Šæ€¥å›æ»¾å®Œæˆ"
```

### ç³»çµ±é‡ç½®

```bash
#!/bin/bash
# ç³»çµ±é‡ç½®è…³æœ¬ï¼ˆè¬¹æ…ä½¿ç”¨ï¼‰

read -p "âš ï¸ é€™å°‡æ¸…é™¤æ‰€æœ‰é·ç§»è³‡æ–™ï¼Œç¢ºå®šè¦ç¹¼çºŒå—ï¼Ÿ(y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "æ“ä½œå·²å–æ¶ˆ"
    exit 1
fi

echo "ğŸ”„ é–‹å§‹ç³»çµ±é‡ç½®..."

# åœæ­¢æ‰€æœ‰ç¨‹åº
pkill -f "python.*memory"

# æ¸…ç†æ¨¡å‹å¿«å–
rm -rf ~/.cache/huggingface/
rm -rf ./data/memory/models/

# é‡ç½®è³‡æ–™åº«
rm -f data/memory/memory.db
rm -rf data/memory/vectors/

# é‡ç½®é…ç½®
if [ -f "settings_qwen3_example.json" ]; then
    cp settings_qwen3_example.json settings.json
fi

echo "âœ… ç³»çµ±é‡ç½®å®Œæˆï¼Œå¯ä»¥é‡æ–°é–‹å§‹é·ç§»"
```

### è¯çµ¡æ”¯æ´

å¦‚æœå•é¡Œç„¡æ³•è§£æ±ºï¼Œè«‹æ”¶é›†ä»¥ä¸‹è³‡è¨Šä¸¦å°‹æ±‚æŠ€è¡“æ”¯æ´ï¼š

1. **ç³»çµ±è³‡è¨Š**:
   ```bash
   uname -a
   python --version
   pip list | grep -E "(torch|transformers|sentence-transformers)"
   ```

2. **éŒ¯èª¤æ—¥èªŒ**:
   ```bash
   tail -50 logs/qwen3_migration_*.log
   ```

3. **é…ç½®æª”æ¡ˆ**:
   ```bash
   cat settings.json
   ```

4. **ç³»çµ±è³‡æº**:
   ```bash
   free -h
   df -h
   nvidia-smi  # å¦‚æœæœ‰ GPU
   ```

---

*å¦‚æœ‰å…¶ä»–å•é¡Œï¼Œè«‹åƒè€ƒé …ç›® README æˆ–æäº¤ GitHub Issue*